# jemdoc: menu{MENU}{research.html}
= Selected Projects

== Robust motion planning with tube-certified trajectory tracking
We propose an approach to guaranteed trajectory tracking for nonlinear control-affine systems subject to external disturbances based on robust control contraction metrics (CCM) that aims to minimize the Linf gain from the disturbances to nominal-actual trajectory deviations. The guarantee is in the form of invariant tubes, computed offline and valid for any nominal trajectories, in which the actual states and inputs of the system are guaranteed to stay despite disturbances. The tracking controller together with tubes can be incorporated into a feedback motion planning framework to plan safe trajectories for robotic systems.
~~~
{}{raw}
<iframe width="560" height="315" src="https://www.youtube.com/embed/mrN5iQo7NxE" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>
~~~
References
- P. Zhao, A. Lakshmanan, K. Ackerman, A. Gahlawat, M. Pavone, and N. Hovakimyan. Tube-certified trajectory tracking for nonlinear systems with robust control contraction metrics. /IEEE Robotics and Automation Letters/, in press, 2022. arXiv:2109.04453.


== Robustifying reinforcement learning policies with adaptive augmentation
 A reinforcement learning (RL) control policy trained in a nominal environment could fail in a new/perturbed environment due to the existence of dynamic variations. For controlling systems with continuous state and action spaces, we propose an add-on approach to robustifying a pre-trained RL policy by augmenting it with an $\mathcal L_1$ adaptive controller (L1AC). Leveraging the capability of an L1AC for fast estimation and active compensation of dynamic variations, the proposed approach can improve the robustness of an RL policy which is trained either in a simulator or in the real world without consideration of a broad class of dynamic variations.
~~~
{}{raw}
<iframe width="560" height="315" src="https://www.youtube.com/embed/xgOB9vpyUgE" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>
~~~
References
- Y. Cheng*, P. Zhao*, F. Wang, D. J. Block, and N. Hovakimyan. Improving the robustness of reinforcement learning policies with $\mathcal L_1$ adaptive control. /IEEE Robotics and Automation Letters/,
under review, 2021. arXiv:2112.01953. (*equal contribution)



== Adaptive control for Learn-to-Fly
Learn-to-Fly  aims to reduce or eliminate the need for ground-based aero-dynamic modeling in favor of in-flight modeling and control law determination. At the initial stage of the model learning process, large uncertainties inevitably exist. To stabilize the aerial vehicles in the presence of the large uncertainties, we leverage and extend the $\mathcal{L}_1$ adaptive control architecture to allow for switched nominal/desired dynamics, as a result of periodic update of the learned model. The framework is validated by flight tests on UAVs.
~~~
{}{raw}
<iframe width="560" height="315" src="https://www.youtube.com/embed/y6O1mwzHdOE" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>
~~~
References 
- S. Snyder, P. Zhao and N. Hovakimyan. $\mathcal L_1$ adaptive control with switched reference models: Application to Learn-to-Fly. /Journal of Guidance, Control, and Dynamics/, revision under review, 2021. arXiv:2108.08462

== Robust and gain-scheduled control of miniaturized optical image stabilizers (OISs)
In this project, robust and gain-scheduled control methods are explored for miniaturized optical image stabilizers to deal with the inevitable product variations. These methods were experimentally validated on both large-scale and small-scale prototypes. 
~~~
{}{img_left}{https://github.com/boranzhao/boranzhao.github.io/blob/main/figs/OIS-ExpSetup.png?raw=true}{Experimental setup with the large-scale prototype}{560}{345}{https://github.com/boranzhao/boranzhao.github.io/blob/main/figs/OIS-ExpSetup.png?raw=true}
~~~
References
- A. Alizadegan, P. Zhao, R. Nagamune and M. Chiao. Experimental validation of a robust $H_\infty$ control method on miniaturized optical image stabilizer prototypes. /Journal of Dynamic Systems, Measurement, and Control/, 142(12): 124501, 2020. 
- P. Zhao, R. Nagamune and M. Chiao. Multiple parameter-dependent robust control of minia- turized optical image stabilizers. /Control Engineering Practice/, 2018, 76: 1–11. 
- A. Alizadegan, P. Zhao, R. Nagamune and M. Chiao. Robust $H_\infty$ control of miniaturized optical image stabilizers against product variabilities. /Control Engineering Practice/, 80: 70–82, 2018. 
- P. Zhao, A. Alizadegan, R. Nagamune and M. Chiao. Robust control of large-scale prototypes for miniaturized optical image stabilizers with product variations. /Proceedings of SICE Annual Conference/, pp. 925–930, 2015. 

