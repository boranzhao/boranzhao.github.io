<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN"
  "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en">
<head>
<meta name="generator" content="jemdoc, see http://jemdoc.jaboc.net/" />
<meta http-equiv="Content-Type" content="text/html;charset=utf-8" />
<link rel="stylesheet" href="jemdoc.css" type="text/css" />
<link rel="shortcut icon" type="image/x-icon" href="favicon.ico" />
<title>Selected Projects</title>
</head>
<body>
<table summary="Table for page layout." id="tlayout">
<tr valign="top">
<td id="layout-menu">
<div class="menu-category">Pan Zhao</div>
<div class="menu-item"><a href="index.html">Home</a></div>
<div class="menu-item"><a href="research.html" class="current">Research</a></div>
<div class="menu-item"><a href="publication.html">Publications</a></div>
<div class="menu-item"><a href="experience.html">Experience</a></div>
<div class="menu-item"><a href="teaching.html">Teaching</a></div>
</td>
<td id="layout-content">
<div id="toptitle">
<h1>Selected Projects</h1>
</div>
<h2>Robust integrated planning and control for nonlinear systems</h2>
<p>We propose an approach to guaranteed trajectory tracking for nonlinear control-affine systems subject to external disturbances based on robust control contraction metrics (CCM) that aims to minimize the Linf gain from the disturbances to nominal-actual trajectory deviations. The guarantee is in the form of invariant tubes, computed offline and valid for any nominal trajectories, in which the actual states and inputs of the system are guaranteed to stay despite disturbances. The tracking controller together with tubes can be incorporated into a feedback motion planning framework to plan safe trajectories for robotic systems.</p>
<iframe width="560" height="315" src="https://www.youtube.com/embed/mrN5iQo7NxE" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>
<p>References</p>
<ul>
<li><p>P. Zhao, A. Lakshmanan, K. Ackerman, A. Gahlawat, M. Pavone, and N. Hovakimyan. Tube-certified trajectory tracking for nonlinear systems with robust control contraction metrics. <i>IEEE Robotics and Automation Letters</i>, 7(2): 5528-5535, 2022. </p>
</li>
</ul>
<h2>Robustifying reinforcement learning policies with adaptive augmentation</h2>
<p>A reinforcement learning (RL) control policy trained in a nominal environment could fail in a new/perturbed environment due to the existence of dynamic variations. For controlling systems with continuous state and action spaces, we propose an add-on approach to robustifying a pre-trained RL policy by augmenting it with an <img class="eq" src="eqs/1144982294-130.png" alt="mathcal L_1" style="vertical-align: -3px" /> adaptive controller (L1AC). Leveraging the capability of an L1AC for fast estimation and active compensation of dynamic variations, the proposed approach can improve the robustness of an RL policy which is trained either in a simulator or in the real world without consideration of a broad class of dynamic variations.</p>
<iframe width="560" height="315" src="https://www.youtube.com/embed/xZBcsNMYK3Y" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>
<p>References (* denotes equal contribution)</p>
<ul>
<li><p>Y. Cheng*, P. Zhao*, F. Wang, D. J. Block, and N. Hovakimyan. Improving the robustness of reinforcement learning policies with <img class="eq" src="eqs/1144982294-130.png" alt="mathcal L_1" style="vertical-align: -3px" /> adaptive control. <i>IEEE Robotics and Automation Letters</i>,
in press, 2022. arXiv:2112.01953.</p>
</li>
<li><p>Y. Cheng*, P. Zhao*, M. Gandhi, B. Li, E. Theodorou and N. Hovakimyan. Robustifying reinforcement learning policies with L1 adaptive control, in <i>Workshop on Safe Robot Control with Learned Motion and Environment Models</i>, <i>International Conference on Robotics and Automation</i>, 2021. </p>
</li>
</ul>
<h2>Adaptive control for Learn-to-Fly</h2>
<p>Learn-to-Fly  aims to reduce or eliminate the need for ground-based aero-dynamic modeling in favor of in-flight modeling and control law determination. At the initial stage of the model learning process, large uncertainties inevitably exist. To stabilize the aerial vehicles in the presence of the large uncertainties, we leverage and extend the <img class="eq" src="eqs/231873739-130.png" alt="mathcal{L}_1" style="vertical-align: -3px" /> adaptive control architecture to allow for switched nominal/desired dynamics, as a result of periodic update of the learned model. The framework is validated by flight tests on UAVs.</p>
<iframe width="560" height="315" src="https://www.youtube.com/embed/y6O1mwzHdOE" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>
<p>References </p>
<ul>
<li><p>S. Snyder, P. Zhao and N. Hovakimyan. <img class="eq" src="eqs/1144982294-130.png" alt="mathcal L_1" style="vertical-align: -3px" /> adaptive control with switched reference models: Application to Learn-to-Fly. <i>Journal of Guidance, Control, and Dynamics</i>, revision under review, 2021. arXiv:2108.08462</p>
</li>
</ul>
<h2>Intelligent and sustainable agricultural management with reinforcement learning</h2>
<p>Soil carbon sequestration in croplands has tremendous potential to help mitigate climate change; however, it is challenging to develop the optimal management practices for maximizing sequestered carbon while improving crop yield. Reinforcement learning (RL) is very promising for solving this crop management problem, which is essentially a sequential decision-making problem. In light of this, this project aims to develop an intelligent agricultural management system using reinforcement learning (RL) and crop simulations. To facilitate training of the RL agents, we will leverage Agricultural Production Systems sIMulator (APSIM) to model and simulate the complex soil-plant-atmosphere interaction. The trained policies are expected to maximize the stored organic carbon while maximizing the crop yield in the presence of uncertain weather conditions. </p>
<table class="imgtable"><tr><td>
<a href="https://github.com/boranzhao/boranzhao.github.io/blob/main/figs/carbon-rl.png?raw=true"><img src="https://github.com/boranzhao/boranzhao.github.io/blob/main/figs/carbon-rl.png?raw=true" alt="Proposed framework for agricultural managment based on RL and crop simulations" width="560px" height="345px" /></a>&nbsp;</td>
<td align="left"></td></tr></table>
<p>References (* denotes equal contribution)</p>
<ul>
<li><p>J. Wu*, R. Tao*, P. Zhao*, N. Martin and N. Hovakimyan. Optimizing nitrogen management with deep reinforcement learning and
crop simulations. IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshop on AGRICULTURE-VISION, accepted, 2022. </p>
</li>
</ul>
<h2>Robust and gain-scheduled control of miniaturized optical image stabilizers (OISs)</h2>
<p>In this project, robust and gain-scheduled control methods are explored for miniaturized optical image stabilizers to deal with the inevitable product variations. These methods were experimentally validated on both large-scale and small-scale prototypes. </p>
<table class="imgtable"><tr><td>
<a href="https://github.com/boranzhao/boranzhao.github.io/blob/main/figs/OIS-ExpSetup.png?raw=true"><img src="https://github.com/boranzhao/boranzhao.github.io/blob/main/figs/OIS-ExpSetup.png?raw=true" alt="Experimental setup with the large-scale prototype" width="560px" height="345px" /></a>&nbsp;</td>
<td align="left"></td></tr></table>
<p>References</p>
<ul>
<li><p>A. Alizadegan, P. Zhao, R. Nagamune and M. Chiao. Experimental validation of a robust <img class="eq" src="eqs/1019227541-130.png" alt="H_infty" style="vertical-align: -4px" /> control method on miniaturized optical image stabilizer prototypes. <i>Journal of Dynamic Systems, Measurement, and Control</i>, 142(12): 124501, 2020. </p>
</li>
<li><p>P. Zhao, R. Nagamune and M. Chiao. Multiple parameter-dependent robust control of minia- turized optical image stabilizers. <i>Control Engineering Practice</i>, 2018, 76: 1–11. </p>
</li>
<li><p>A. Alizadegan, P. Zhao, R. Nagamune and M. Chiao. Robust <img class="eq" src="eqs/1019227541-130.png" alt="H_infty" style="vertical-align: -4px" /> control of miniaturized optical image stabilizers against product variabilities. <i>Control Engineering Practice</i>, 80: 70–82, 2018. </p>
</li>
<li><p>P. Zhao, A. Alizadegan, R. Nagamune and M. Chiao. Robust control of large-scale prototypes for miniaturized optical image stabilizers with product variations. <i>Proceedings of SICE Annual Conference</i>, pp. 925–930, 2015. </p>
</li>
</ul>
<div id="footer">
<div id="footer-text">
Page generated 2022-04-25 14:46:29 Central Daylight Time, by <a href="http://jemdoc.jaboc.net/">jemdoc</a>.
</div>
</div>
</td>
</tr>
</table>
</body>
</html>
