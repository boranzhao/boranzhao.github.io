<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN"
  "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en">
<head>
<meta name="generator" content="jemdoc, see http://jemdoc.jaboc.net/" />
<meta http-equiv="Content-Type" content="text/html;charset=utf-8" />
<link rel="stylesheet" href="jemdoc.css" type="text/css" />
<link rel="shortcut icon" type="image/x-icon" href="favicon.ico" />
<title>Selected Projects</title>
</head>
<body>
<table summary="Table for page layout." id="tlayout">
<tr valign="top">
<td id="layout-menu">
<div class="menu-category">Pan Zhao</div>
<div class="menu-item"><a href="index.html">Home</a></div>
<div class="menu-item"><a href="research.html" class="current">Research</a></div>
<div class="menu-item"><a href="publication.html">Publications</a></div>
<div class="menu-item"><a href="experience.html">Experience</a></div>
<div class="menu-item"><a href="teaching.html">Teaching</a></div>
</td>
<td id="layout-content">
<div id="toptitle">
<h1>Selected Projects</h1>
</div>
<h2>Robust reinforcement learning</h2>
<p>A reinforcement learning (RL) control policy trained in a nominal environment could fail in a new/perturbed environment due to the existence of dynamic variations. For controlling systems with continuous state and action spaces, we propose an add-on approach to robustifying a pre-trained RLpolicy by augmenting it with an <img class="eq" src="eqs/1144982294-130.png" alt="mathcal L_1" style="vertical-align: -3px" /> adaptive controller (L1AC). Leveraging the capability of an L1AC for fast estimation and active compensation of dynamic variations, the proposed approach can improve the robustness of an RL policy which is trained either in a simulator or in the real world without consideration of a broad class of dynamic variations.</p>
<iframe width="560" height="315" src="https://www.youtube.com/embed/xgOB9vpyUgE" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>
<p>References</p>
<ul>
<li><p>Y. Cheng<b>, P. Zhao</b>, F. Wang, D. J. Block, and N. Hovakimyan. Improving the Robustness of reinforcement learning Policies with <img class="eq" src="eqs/1144982294-130.png" alt="mathcal L_1" style="vertical-align: -3px" /> adaptive control. <i>IEEE Robotics and Automation Letters</i>,
under review, 2021. arXiv:2112.01953. (*equal contribution)</p>
</li>
</ul>
<h2>Adaptive control for Learn-to-Fly</h2>
<p>Learn-to-Fly  aims to reduce or eliminate the need for ground-based aero-dynamic modeling in favor of in-flight modeling and control law determination. At the initial stage of the model learning process, large uncertainties inevitably exist. To stabilize the aerial vehicles in the presence of the large uncertainties, we leverage and extend the <img class="eq" src="eqs/231873739-130.png" alt="mathcal{L}_1" style="vertical-align: -3px" /> adaptive control architecture to allow for switched nominal/desired dynamics, as a result of periodic update of the learned model. The framework is validated by flight tests on UAVs.</p>
<iframe width="560" height="315" src="https://www.youtube.com/embed/y6O1mwzHdOE" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>
<p>References </p>
<ul>
<li><p>S. Snyder, P. Zhao and N. Hovakimyan. <img class="eq" src="eqs/1144982294-130.png" alt="mathcal L_1" style="vertical-align: -3px" /> adaptive control with switched reference models: Application to Learn-to-Fly. <i>Journal of Guidance, Control, and Dynamics</i>, revision under review, 2021. arXiv:2108.08462</p>
</li>
</ul>
<h2>Robust and gain-scheduled control of miniaturized optical image stabilizers (OISs)</h2>
<p>In this project, robust and gain-scheduled control methods are explored for miniaturized optical image stabilizers to deal with the inevitable product variations. These methods were experimentally validated on both large-scale and small-scale prototypes. </p>
<table class="imgtable"><tr><td>
<a href="https://github.com/boranzhao/boranzhao.github.io/blob/main/figs/OIS-ExpSetup.png?raw=true"><img src="https://github.com/boranzhao/boranzhao.github.io/blob/main/figs/OIS-ExpSetup.png?raw=true" alt="Experimental setup with the large-scale prototype" width="560px" height="345px" /></a>&nbsp;</td>
<td align="left"></td></tr></table>
<ul>
<li><p>A. Alizadegan, P. Zhao, R. Nagamune and M. Chiao. Experimental validation of a robust <img class="eq" src="eqs/1019227541-130.png" alt="H_infty" style="vertical-align: -4px" /> control method on miniaturized optical image stabilizer prototypes. <i>Journal of Dynamic Systems, Measurement, and Control</i>, 142(12): 124501, 2020. </p>
</li>
<li><p>P. Zhao, R. Nagamune and M. Chiao. Multiple parameter-dependent robust control of minia- turized optical image stabilizers. <i>Control Engineering Practice</i>, 2018, 76: 1–11. </p>
</li>
<li><p>A. Alizadegan, P. Zhao, R. Nagamune and M. Chiao. Robust <img class="eq" src="eqs/1019227541-130.png" alt="H_infty" style="vertical-align: -4px" /> control of miniaturized optical image stabilizers against product variabilities. <i>Control Engineering Practice</i>, 80: 70–82, 2018. </p>
</li>
<li><p>P. Zhao, A. Alizadegan, R. Nagamune and M. Chiao. Robust control of large-scale prototypes for miniaturized optical image stabilizers with product variations. <i>Proceedings of SICE Annual Conference</i>, pp. 925–930, 2015. </p>
</li>
</ul>
<div id="footer">
<div id="footer-text">
Page generated 2022-01-01 12:31:22 Central Standard Time, by <a href="http://jemdoc.jaboc.net/">jemdoc</a>.
</div>
</div>
</td>
</tr>
</table>
</body>
</html>
